import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OrdinalEncoder,  StandardScaler
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import random
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer
compas = pd.read_csv('compas-scores.csv')
compas = compas.drop_duplicates(subset = compas.columns.values.tolist()[1:], keep='first')
compas = compas.fillna("0")
compas = compas[compas['is_recid'] != -1]
columns_drop = ['id', 'name', 'first', 'last', 'compas_screening_date', 'dob', 'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number', 'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'r_case_number', 'r_charge_degree', 'r_days_from_arrest', 'r_offense_date', 'r_charge_desc', 'r_jail_in', 'r_jail_out', 'is_violent_recid', 'num_vr_cases', 'vr_case_number', 'vr_offense_date', 'vr_charge_degree' , 'vr_charge_desc', 'screening_date', 'v_screening_date','num_r_cases','decile_score.1','c_charge_desc']
compas = compas.drop(columns=columns_drop, errors = 'ignore')
[train, test] = train_test_split(compas,test_size=0.3)
X_train = train.drop('is_recid',axis=1) 
y_train =train['is_recid']
X_test = test.drop('is_recid',axis=1)
y_test = test['is_recid'] 
ordinal_cat_cols = [ 'age_cat', 'c_charge_degree','decile_score','v_decile_score','v_score_text','score_text'] 
oe = OrdinalEncoder() 
ohe = OneHotEncoder(handle_unknown='ignore') #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
onehot_cat_cols = ['sex','race','type_of_assessment','v_type_of_assessment']
numerical_cols =['age', 'juv_fel_count','juv_misd_count','juv_other_count','priors_count'] 
ie = SimpleImputer(missing_values=np.nan,strategy='mean')
scaler=StandardScaler() #https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976
preprocessor= ColumnTransformer( 
    transformers = [
        ('cat',ohe,onehot_cat_cols),
        ('ord',oe,ordinal_cat_cols),
        ('num', ie, numerical_cols),
        ])
rf_pipeline = Pipeline(steps =[
    ('preprocessor',preprocessor),
    ('rf', RandomForestRegressor(n_estimators =100,random_state=42))
    ]) 
rf_pipeline.fit(X_train,y_train.ravel()) # #https://stackoverflow.com/questions/67828280/why-do-i-need-to-use-ravel-in-this-case


y_pred_rf = np.round(rf_pipeline.predict(X_test))
conf_mat_rf = confusion_matrix(y_test, y_pred_rf) ##https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
print(classification_report(y_test,y_pred_rf)) #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html

lr_pipeline = Pipeline(steps = [
    ('preprocessor',preprocessor),
    ('lr', LogisticRegression(max_iter=1000, random_state=42))
    ])
 #https://scikit-learn.org/stable/modules/compose.html
lr_pipeline.fit(X_train,y_train.ravel())
y_pred_lr = lr_pipeline.predict(X_test)
conf_mat_lr = confusion_matrix(y_test, y_pred_lr) ##https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
print(classification_report(y_test,y_pred_lr))

y_pred_rf = np.round(rf_pipeline.predict(X_test))
compas_pred_rf = pd.DataFrame({'Race': compas.loc[X_test.index, 'race'],
                           'Score Text': compas.loc[X_test.index, 'score_text'],
                           'Actual': y_test,
                           'Predicted': y_pred_rf})

y_pred_lr = lr_pipeline.predict(X_test)
compas_pred_lr = pd.DataFrame({'Race': compas.loc[X_test.index, 'race'],
                           'Score Text': compas.loc[X_test.index, 'score_text'],
                           'Actual': y_test,
                           'Predicted': y_pred_lr})

compas_pred_rf_sorted = compas_pred_rf.sort_values(['Race', 'Score Text'])
compas_pred_lr_sorted = compas_pred_lr.sort_values(['Race', 'Score Text'])
unique_races = compas['race'].unique()

for race in unique_races:
   
    compas_race_rf = compas_pred_rf_sorted[compas_pred_rf_sorted['Race'] == race]
    conf_mat_rf_race = confusion_matrix(compas_race_rf['Actual'], compas_race_rf['Predicted'])
    report_rf_race = classification_report(compas_race_rf['Actual'], compas_race_rf['Predicted'])
    
    print(f"Race: {race} - RandomForestRegressor")
    print("Confusion Matrix:")
    print(conf_mat_rf_race)
    print("Classification Report:")
    print(report_rf_race)
    
  
    compas_race_lr = compas_pred_lr_sorted[compas_pred_lr_sorted['Race'] == race]
    conf_mat_lr_race = confusion_matrix(compas_race_lr['Actual'], compas_race_lr['Predicted'])
    report_lr_race = classification_report(compas_race_lr['Actual'], compas_race_lr['Predicted'])
    
    print(f"Race: {race} - LogisticRegression")
    print("Confusion Matrix:")
    print(conf_mat_lr_race)
    print("Classification Report:")
    print(report_lr_race)
    print("\n")
group_counts = compas['race'].value_counts()
positive_outcomes = compas[compas['is_recid'] == 1]['race'].value_counts()
negative_outcomes = compas[compas['is_recid'] == 0]['race'].value_counts()
group_proportions = positive_outcomes / group_counts
overall_positive_proportion = compas[compas['is_recid'] == 1]['race'].count() / compas['race'].count()
disparate_impact = group_proportions / overall_positive_proportion
print(disparate_impact)
group_counts = compas['c_charge_degree'].value_counts()
positive_outcomes = compas[compas['is_recid'] == 1]['c_charge_degree'].value_counts()
negative_outcomes = compas[compas['is_recid'] == 0]['c_charge_degree'].value_counts()
group_proportions = positive_outcomes / group_counts
overall_positive_proportion = compas[compas['is_recid'] == 1]['c_charge_degree'].count() / compas['c_charge_degree'].count()
disparate_impact = group_proportions / overall_positive_proportion
print(disparate_impact)
